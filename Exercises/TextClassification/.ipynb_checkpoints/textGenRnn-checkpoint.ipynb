{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset if not already\n",
    "path = r'C:\\Users\\pmspr\\Documents\\Machine Learning\\Courses\\Tensorflow Cert\\Data'\n",
    "folder = 'nlp'\n",
    "abs_path = os.path.join(path, folder)\n",
    "abs_path = os.path.join(abs_path, 'shakespeare')\n",
    "if not os.path.exists(os.path.join(abs_path, 'shakespeare.txt')):\n",
    "    sp_txt = tf.keras.utils.get_file('shakespeare.txt',\n",
    "                                       cache_subdir=abs_path,\n",
    "                                       origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
    "                                       )\n",
    "    sp_dir = abs_path\n",
    "else:\n",
    "    sp_dir = abs_path\n",
    "\n",
    "file = os.path.join(sp_dir, 'shakespeare.txt')\n",
    "text = open(file, 'rb').read().decode(encoding='utf-8')\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:250])\n",
    "\n",
    "corpus = text.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to clean the text from punctuation\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                    '[%s]' % re.escape(string.punctuation),\n",
    "                                    '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize layer consider words, but in this example we are using character level prediction.\n",
    "max_features = 30000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(np.asarray(corpus))\n",
    "\n",
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)\n",
    "\n",
    "# Only dataset.batch can be used in this way. Or we need to create a model for vectorization layer and use model.add \n",
    "# and model.predcit to get the output.\n",
    "#po = [vectorize_text(tf.constant(x)) for x in corpus] This doesnt work.\n",
    "ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "ds = ds.batch(2).map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'StringLookup'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ad862f3f161e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use string lookup to map each different vocab to a number id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m ids_from_chars = preprocessing.StringLookup(\n\u001b[0m\u001b[0;32m      3\u001b[0m     vocabulary=list(vocab))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'StringLookup'"
     ]
    }
   ],
   "source": [
    "# Use string lookup to map each different vocab to a number id\n",
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
